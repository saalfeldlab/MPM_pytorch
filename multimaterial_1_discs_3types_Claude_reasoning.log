
============================================================
=== Iteration 1 ===
============================================================
**Summary of Iteration 1 Analysis:**

## Iter 1: POOR (R²=0.416)
- **Node:** id=1, parent=root
- **Config:** hidden_dim=64, omega_f=80, lr=1E-5, 50k steps, 100 frames
- **Diagnosis:** Severe underfit - model too small (6x smaller than known Jp optimal), omega_f too high (4x higher than optimal)

## Next Iteration Setup:
- **Parent:** Node 1 (highest UCB)
- **Strategy:** exploit
- **Mutation:** hidden_dim_nnr_f: 64 → 256 (4x increase towards optimal 384)
- **Hypothesis:** Increasing model capacity from 12.8k to ~200k parameters should dramatically improve R²

Config has been updated. Ready for next iteration.


============================================================
=== Iteration 2 ===
============================================================
**Iteration 2 Analysis Complete**

**Results Summary:**
- R² improved from 0.416 → 0.799 (+0.383) by increasing hidden_dim from 64 → 256
- Still in "moderate" category (R² 0.75-0.90)
- Slope = 0.189 indicates severe underprediction (should be ~1.0)
- omega_f=80 is clearly too high for Jp@100frames

**Next Iteration (Iter 3):**
- Parent: Node 2 (highest UCB=1.799)
- Strategy: exploit
- Mutation: omega_f: 80 → 25 (moving toward known optimal range of 20-25 for Jp@100frames)
- Hypothesis: Reducing omega_f should dramatically improve R² and slope, potentially achieving R² > 0.95


============================================================
=== Iteration 3 ===
============================================================
**Iteration 3 Summary:**

- **Results**: R²=0.774 (moderate), slope=0.178, SSIM=0.998, training_time=2.7min
- **Key finding**: Reducing omega_f from 80→25 did NOT improve R² (-0.025). This suggests the learning rate (1E-5) is the primary bottleneck, not omega_f.
- **Visual analysis**: Excellent spatial match (SSIM=0.998) but severe underprediction - the model predicts a narrow range around 1.0 while GT spans 0.5-2.0.
- **UCB selection**: Node 3 has highest UCB (1.999)
- **Next iteration (4)**: lr_NNR_f: 1E-5 → 3E-5 (3x increase towards prior optimal of 4E-5)

Config updated and ready for iteration 4.


============================================================
=== Iteration 4 ===
============================================================
## Iteration 4 Summary

**Results**: R²=0.873 (moderate), slope=0.455, training_time=2.5min

**Key finding**: Learning rate was the bottleneck. Increasing lr_NNR_f from 1E-5 to 3E-5 improved R² by +0.099 and slope by 2.6x (0.178→0.455). The model is now capturing more of the dynamic range.

**Progress trajectory**:
- Iter 1: R²=0.416 (poor) - baseline with undersized model
- Iter 2: R²=0.799 (moderate) - capacity increase helped
- Iter 3: R²=0.774 (moderate) - omega_f change didn't help at low lr
- Iter 4: R²=0.873 (moderate) - lr increase confirmed as bottleneck

**Next iteration (5)**: Exploit from Node 4 (highest UCB=2.287). Increase hidden_dim from 256 to 384 (the known optimal for Jp field). This should push us toward R²>0.90.


============================================================
=== Iteration 5 ===
============================================================
**Iteration 5 Summary:**

| Metric | Value |
|--------|-------|
| R² | 0.867 (moderate) |
| Slope | 0.634 |
| Training time | 3.3 min |
| Params | 445,441 |

**Analysis:**
- Node 5 (hidden_dim=384) has highest UCB (2.448)
- hidden_dim 256→384 gave mixed results: R² slightly worse (-0.006) but slope much better (+0.179)
- The larger model captures dynamic range better (slope 0.634 vs 0.455) but needs more training steps
- Current 50k steps = 500 steps/frame, which is below the ~1000 steps/frame needed for R²>0.99

**Next iteration config:**
- parent=5, strategy=exploit
- Mutation: total_steps 50000 → 80000 (800 steps/frame)
- Hypothesis: More training should help the 384-dim model converge and push R² above 0.9


============================================================
=== Iteration 6 ===
============================================================
**Iteration 6 Summary:**

**Results**: R²=0.937 (good), slope=0.813, training_time=5.0min

**Key findings**:
- total_steps 50k→80k: +0.070 R², +0.179 slope - confirmed larger models need more steps
- Now approaching excellent threshold (R²>0.95)
- 800 steps/frame is effective for 384-dim model

**Next iteration (7)**:
- Parent: Node 6 (highest UCB=2.669)
- Mutation: lr_NNR_f: 3E-5 → 4E-5 (prior knowledge indicates 4E-5 is optimal for Jp)
- Hypothesis: Higher learning rate will push R² above 0.95 excellent threshold

Config is ready for iteration 7 with lr_NNR_f=4E-5.


============================================================
=== Iteration 7 ===
============================================================
The configuration is set for the next iteration:

**Iteration 7 Summary:**
- **Result: EXCELLENT** (R²=0.960, slope=0.848, SSIM=1.000)
- Mutation: lr_NNR_f 3E-5 → 4E-5 improved both R² (+0.023) and slope (+0.035)
- First excellent result in this block - confirms prior knowledge that lr_NNR_f=4E-5 is optimal for Jp field

**Iteration 8 Configuration:**
- **Parent**: Node 7 (highest UCB=2.831)
- **Strategy**: exploit
- **Mutation**: omega_f 25.0 → 20.0
- **Hypothesis**: Prior knowledge suggests omega_f=20-25 optimal for Jp@100frames. Testing lower bound may improve slope (currently 0.848, target ~1.0)

Config for next iteration:
- lr_NNR_f: 4E-5
- hidden_dim_nnr_f: 384
- n_layers_nnr_f: 3
- omega_f: 20.0 (changed from 25.0)
- total_steps: 80000
- batch_size: 1
- n_training_frames: 100


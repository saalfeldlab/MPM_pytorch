
============================================================
=== BATCH 0 (start call) ===
============================================================
All 4 config files are initialized and ready. Here's the summary:

**Block 1: F field @ 400 frames (siren_txy)**

| Slot | omega_f | lr_NNR_f | hidden_dim | n_layers | total_steps | Dimension tested |
|------|---------|----------|------------|----------|-------------|------------------|
| 00 | 8.0 | 5E-5 | 256 | 4 | 320k | **Baseline** (appendix reference) |
| 01 | 6.0 | 5E-5 | 256 | 4 | 320k | **omega_f** (probe continued scaling) |
| 02 | 8.0 | 8E-5 | 256 | 4 | 320k | **lr** (data regularization at 400f) |
| 03 | 8.0 | 5E-5 | 256 | 3 | 400k | **depth** (shallower + more steps) |

**Rationale**: F is the most scalable field (no diminishing returns to 500f). Each slot varies exactly one parameter dimension from the appendix baseline, enabling clean causal attribution. The `dataset` fields are preserved unchanged in all configs. Working memory and analysis log have been updated with the planned configurations and hypotheses.


============================================================
=== Batch 1-4 ===
============================================================
## Batch 1 Analysis Complete (Iterations 1-4)

### Results Summary

| Slot | Iter | Mutation | R² | slope | kino_SSIM | Rating |
|------|------|----------|-----|-------|-----------|--------|
| 00 | 1 | Baseline (omega_f=8, lr=5E-5) | 0.9998 | 0.9997 | 0.984 | Excellent |
| 01 | 2 | omega_f=6 | 0.9996 | 0.9995 | 0.974 | Excellent |
| 02 | 3 | **lr=8E-5** | **0.9999** | **0.9999** | **0.992** | **Excellent (BEST)** |
| 03 | 4 | depth=3, steps=400k | 0.9990 | 0.9989 | 0.953 | Good |

### Key Findings
- **F scales to 400 frames** — all 4 configs achieve R²>0.999
- **lr=8E-5 is best** (Node 3): data regularization at 400f allows higher lr, consistent with Jp pattern
- **omega_f scaling plateaus**: 12(100f)→9(200f)→8(400f), NOT linear. omega_f=6 is too low.
- **Depth=4 mandatory**: n_layers=3 loses ~0.0008 R² even with 25% more steps

### Batch 2 Configs (Iterations 5-8) — All exploit from Node 3

| Slot | Parent | Mutation | omega_f | lr | steps | Rationale |
|------|--------|----------|---------|-----|-------|-----------|
| 00 | 3 | lr probe up | 8.0 | **1E-4** | 320k | Test lr upper boundary |
| 01 | 3 | omega_f+lr | **10.0** | **8E-5** | 320k | Probe omega_f upward (since 8>6) |
| 02 | 3 | more steps | 8.0 | 8E-5 | **400k** | Loss was still declining at 320k |
| 03 | 3 | aggressive lr | 8.0 | **1.2E-4** | 320k | Find lr ceiling |


============================================================
=== Batch 5-8 ===
============================================================
All tasks complete. Here's the summary:

---

## Block 1 Complete — F@400f siren_txy (8 iterations)

### Batch 2 Results (Iterations 5-8)

| Iter | Mutation | R² | MSE | kino_SSIM | Time |
|------|----------|-----|-----|-----------|------|
| 5 | lr: 8E-5→1E-4 | 0.999928 | 3.46E-5 | 0.9946 | 18.4min |
| 6 | omega_f: 8→10 | 0.999930 | 3.34E-5 | 0.9946 | 18.5min |
| 7 | steps: 320k→400k | 0.999918 | 3.95E-5 | 0.9939 | 22.9min |
| **8** | **lr: 8E-5→1.2E-4** | **0.999950** | **2.42E-5** | **0.9962** | **18.4min** |

### Key Findings
- **Iter 8 (lr=1.2E-4) = NEW BEST**: R²=0.99995, lowest MSE, highest kino_SSIM
- **omega_f flat in [8-10]**: omega_f=10 ≈ omega_f=8 (insensitive)
- **400k steps OVERTRAINS**: worse than 320k at lr=8E-5 (+4.5min for worse result)
- **lr ceiling still not found**: 1.2E-4 is best — could go higher

### F@400f Complete Map
- omega_f: 6(0.9996) < **8-10(0.9999)** — flat optimum, plateau reached
- lr: 5E-5 < 8E-5 < 1E-4 < **1.2E-4(0.99995)** — ceiling rising with data
- depth: 3(0.999) << **4(0.99995)** — mandatory
- steps: **320k(800/f)** > 400k(1000/f) — overtrains

### Block-End Actions
1. **Instructions edited**: Added F@400f complete map, omega_f plateau rule, lr-data scaling confirmation, overtraining rule
2. **Next block**: Switched to **Jp@400f siren_txy** (Block 2, iterations 9-16)
3. **Memory updated**: Regime table updated, previous block summarized, new block hypothesis written

### Next Block Configs (Jp@400f)

| Slot | omega_f | lr | hidden_dim | n_layers | steps | Dimension |
|------|---------|-----|------------|----------|-------|-----------|
| 00 | 5.0 | 1.5E-4 | 512 | 3 | 600k | Baseline (appendix) |
| 01 | 3.0 | 1.5E-4 | 512 | 3 | 600k | omega_f lower |
| 02 | 5.0 | 2E-4 | 512 | 3 | 600k | lr higher |
| 03 | 5.0 | 1.5E-4 | 384 | 3 | 600k | capacity (speed Pareto test) |


============================================================
=== Batch 9-12 ===
============================================================
All tasks complete. Here's the summary:

---

**Batch 1 Results (Block 2, Jp@400f siren_txy, iters 9-12):**

All 4 slots achieved excellent results (R² > 0.99998). Jp scales very well to 400 frames.

| Iter | Config | R² | MSE | Time | Key finding |
|------|--------|----|-----|------|-------------|
| 9 | Baseline (omega=5, lr=1.5E-4, 512×3) | 0.999992 | 2.17E-3 | 38.7min | Appendix reference works well |
| 10 | omega_f=3 | 0.999982 | 4.58E-3 | 38.9min | omega_f=3 worse than 5 (2× MSE) |
| **11** | **lr=2E-4** | **0.999996** | **9.68E-4** | **39.0min** | **NEW BEST — lr ceiling still rising** |
| 12 | hidden_dim=384 | 0.999986 | 3.50E-3 | 26.2min | Speed Pareto confirmed (32% faster) |

**Key findings:** lr=2E-4 best (lr ceiling progression: 4E-5@100f → 1E-4@200f → 2E-4@400f). omega_f=5 > 3. 384 is viable speed Pareto. All kinograph metrics saturated at 1.0000.

**Next batch (iters 13-16) configs set:**

| Slot | Mutation from parent | Strategy |
|------|---------------------|----------|
| 00 | lr=2.5E-4 (from iter 11) | exploit: lr ceiling probe |
| 01 | omega_f=7, lr=2E-4 (from iter 11) | exploit: omega_f upper boundary |
| 02 | 400k steps, lr=2E-4 (from iter 11) | explore: step reduction for speed |
| 03 | 384×3, lr=2E-4 (from iter 12) | principle-test: 384+high lr combo |


============================================================
=== Batch 13-16 ===
============================================================
**Block 2 complete. Summary of iterations 13-16 (Jp@400f):**

| Iter | Mutation | R² | MSE | slope | Time | Finding |
|------|----------|-----|-----|-------|------|---------|
| 13 | lr 2E-4→2.5E-4 | 0.99985 | 4.4E-2 | 0.995 | 38.7min | lr CEILING FOUND — 2.5E-4 overshoots (MSE 45× worse) |
| 14 | omega_f 5→7 | 0.99995 | 1.4E-2 | 0.999 | 38.9min | omega_f=5 LOCAL MAX — 7 worse (MSE 14×) |
| 15 | steps 600k→400k | 0.99999 | 3.7E-3 | 0.999 | 26.2min | 400k viable speed Pareto (33% faster, negligible loss) |
| 16 | 384@lr=2E-4 | 0.99999 | 1.3E-3 | 0.9999 | 26.3min | **BEST EFFICIENCY** — 384+lr=2E-4 matches 512 |

**Block 2 key results (Jp@400f siren_txy):**
- Best accuracy: 512×3, omega_f=5, lr=2E-4, 600k steps → R²=0.999996, 39min
- Best efficiency: 384×3, omega_f=5, lr=2E-4, 600k steps → R²=0.999995, 26.3min
- lr ceiling at 2E-4 (scaling: 4E-5→1E-4→2E-4 from 100→200→400f)
- omega_f=5 is narrow peak (unlike F's flat [8-10])

**Block 3 set up: C@400f siren_txy** (iterations 17-24)

| Slot | omega_f | lr | hidden_dim | Dimension tested |
|------|---------|-----|------------|------------------|
| 00 | 18.0 | 4E-5 | 896 | Baseline (appendix) |
| 01 | 22.0 | 4E-5 | 896 | omega_f (higher) |
| 02 | 18.0 | 6E-5 | 896 | lr (higher, test data regularization) |
| 03 | 18.0 | 4E-5 | 768 | capacity (principle-test: does C need 896?) |

All configs: siren_txy, batch_size=1, n_training_frames=400, 1M steps, n_layers=3.


============================================================
=== Batch 17-20 ===
============================================================
All tasks complete. Here's the summary:

## Batch 1 Results (Iterations 17-20): C@400f siren_txy

**Major finding: C@400f dramatically exceeds predictions** — R²=0.9998 vs predicted ~0.99. The prior claim that "C HURTS with more data" is **reversed** at 400f with sufficient capacity (896×3) and training steps (1M). Scaling: 0.994(100f) → 0.991(200f) → **0.9998(400f)**.

| Iter | Mutation | R² | MSE | Time | Key Result |
|------|----------|-----|-----|------|------------|
| 17 | **Baseline** (omega_f=18, lr=4E-5, 896×3) | **0.999758** | 5.13E-2 | 156.1min | **BEST** — appendix prediction accurate |
| 18 | omega_f=22 | 0.999650 | 7.42E-2 | 155.7min | omega_f=22 worse; trend 25→20→18 confirmed |
| 19 | lr=6E-5 | 0.999734 | 5.62E-2 | 155.9min | lr=6E-5 marginally worse; C lr-scaling weaker than F/Jp |
| 20 | hidden_dim=768 | 0.999729 | 5.74E-2 | 120.6min | **SPEED PARETO** — 0.003% R² loss, 23% time saving |

## Next Batch (Iterations 21-24): Boundary probing

| Slot | Config Change | Parent | Rationale |
|------|--------------|--------|-----------|
| 00 | omega_f=15 | 17 | Probe omega_f lower boundary |
| 01 | lr=3E-5 | 17 | Probe lr lower boundary |
| 02 | 750k steps (from 1M) | 17 | Test step reduction for time savings |
| 03 | hidden_dim=640 | 20 | Principle-test: does C@100f capacity ceiling (640) hold at 400f? |


============================================================
=== Batch 21-24 ===
============================================================
## Summary

### Batch 2 Results (Iterations 21-24) — Block 3 END

| Slot | Iter | Mutation | R² | MSE | Time | Result |
|------|------|----------|-----|-----|------|--------|
| 00 | 21 | omega_f: 18→15 | **0.999807** | 4.09E-2 | 155.8min | **NEW BEST** |
| 01 | 22 | lr: 4E-5→3E-5 | 0.999692 | 6.56E-2 | 155.7min | Worse — lr too low |
| 02 | 23 | steps: 1M→750k | 0.999521 | 1.02E-1 | 117.2min | Worse — undertrained |
| 03 | 24 | capacity: 896→640 | 0.999600 | 8.51E-2 | 88.4min | Worse — insufficient capacity |

### Block 3 Complete Map (C@400f siren_txy)
- **omega_f**: 15(0.9998) > 18(0.9998) > 22(0.9997). Best=15, continues downward trend.
- **lr**: 3E-5(0.9997) < **4E-5(0.9998)** >= 6E-5(0.9997). Optimal=4E-5.
- **steps**: 750k(0.9995) << **1M(0.9998)**. Needs 2500 steps/frame minimum.
- **capacity**: 640(0.9996) < 768(0.9997) < **896(0.9998)**. 768=speed Pareto.

### Key Block 3 Finding
C@400f **REVERSES** the prior degradation trend: 0.994(100f) → 0.991(200f) → **0.9998(400f)**. The claim "C HURTS with more data" is wrong at 400f with sufficient capacity and training steps.

### Block 4 Setup: S@400f siren_txy
New block explores the hardest field (S) at 400 frames. CosineAnnealingLR + gradient clipping already in code.

| Slot | omega_f | lr | hidden_dim | Role |
|------|---------|-----|------------|------|
| 00 | 48 | 2E-5 | 1280 | Baseline (S@100f extrapolated) |
| 01 | 36 | 2E-5 | 1280 | omega_f lower (all-field downward trend) |
| 02 | 48 | 3E-5 | 1280 | lr higher (data regularization test) |
| 03 | 48 | 2E-5 | 1024 | Capacity lower (principle test) |


============================================================
=== Batch 25-28 ===
============================================================
**Batch 1 Analysis Complete (Iterations 25-28, S@400f)**

**Results Summary:**

| Slot | Iter | Mutation | R² | slope | kino_R2 | Time |
|------|------|----------|----|-------|---------|------|
| 00 | 25 | **Baseline** (omega=48, lr=2E-5, 1280×3) | **0.960** | 0.961 | 0.920 | 290min |
| 01 | 26 | omega_f: 48→36 | 0.949 | 0.949 | 0.898 | 290min |
| 02 | 27 | lr: 2E-5→3E-5 | 0.803 | 0.806 | 0.470 | 290min |
| 03 | 28 | capacity: 1280→1024 | 0.918 | 0.917 | 0.831 | 190min |

**Key findings:**
1. **S@400f baseline R²=0.960** — huge improvement over S@100f no-scheduler (0.729). Data scaling HELPS S substantially.
2. **omega_f=48 confirmed** — S does NOT follow the all-field omega_f decrease. 36 is worse.
3. **lr=2E-5 hard-locked** — even 50% increase (3E-5) is catastrophic (R²=0.803). S fundamentally different from F/Jp/C.
4. **1280 capacity required** — 1024 loses 4.4% R² with steep capacity dependence.

**Next Batch (Iterations 29-32) Configs Set:**

| Slot | Mutation | Parent |
|------|----------|--------|
| 00 | omega_f=55 (upper probe) | 25 |
| 01 | total_steps=1.5M (more training, loss declining) | 25 |
| 02 | lr=1.5E-5 (lower lr probe) | 25 |
| 03 | omega_f=42 (refine map, principle-test) | 25 |


============================================================
=== Batch 29-32 ===
============================================================
All tasks complete. Here's the summary:

## Batch Analysis (Iterations 29-32) — Block 4 END

### Results

| Slot | Iter | Mutation | R² | kino_R2 | Time |
|------|------|----------|-----|---------|------|
| 00 | 29 | omega_f: 48→**55** | **0.970** | 0.940 | 290min |
| 01 | 30 | steps: 1M→1.5M | 0.963 | 0.922 | 435min |
| 02 | 31 | lr: 2E-5→1.5E-5 | 0.965 | 0.933 | 291min |
| 03 | 32 | omega_f: 48→42 | 0.952 | 0.906 | 290min |

### Key Findings
- **Iter 29 is BEST IN BLOCK** (R²=0.970): omega_f=55 beats all previous. S omega_f map: 36→42→48→55, nearly linear **upward** trend — S is the ONLY field where omega_f INCREASES with more frames.
- **1.5M steps OVERTRAINS** (Iter 30): CosineAnnealingLR reaches T_max at 1M, extra steps are wasted.
- **lr=1.5E-5 slightly better** (Iter 31): R²=0.965 vs 0.960 at omega_f=48. S lr has mild low-end flexibility.
- **omega_f=42 worse** (Iter 32): Confirms monotonic upward omega_f trend for S.

### Block 4 Summary
S@400f best: 1280×3@omega_f=55@lr=2E-5@1M steps, R²=0.970, 290min. Data scaling HELPS S enormously (+0.241 vs S@100f). omega_f counter-trend confirmed and strengthened.

### Block 5 Setup: F@600 frames
Pushing to 600 frames with F (most scalable field). 4 slots test: baseline (omega_f=8, lr=1.2E-4, 420k steps), omega_f=7, lr=1.8E-4, and 360k steps (600/f efficiency test).


============================================================
=== Batch 33-36 ===
============================================================
Done. Here's the summary:

## Batch 1 Results (Iterations 33-36): F@600f siren_txy

| Iter | Mutation | R² | MSE | kino_R2 | kino_SSIM | Time |
|------|----------|-----|-----|---------|-----------|------|
| 33 | Baseline (omega=8, lr=1.2E-4, 700/f) | 0.999940 | 2.90E-5 | 0.9999 | 0.9954 | 24.2min |
| 34 | omega_f=7 | 0.999919 | 3.90E-5 | 0.9999 | 0.9940 | 24.4min |
| **35** | **lr=1.8E-4** | **0.999970** | **1.44E-5** | **1.0000** | **0.9976** | **24.4min** |
| 36 | 360k steps (600/f) | 0.999922 | 3.74E-5 | 0.9999 | 0.9941 | 20.9min |

**Key findings:**
- **lr=1.8E-4 is NEW BEST** (R²=0.99997, MSE 2× lower than baseline). lr ceiling continues rising: 5E-5(200f) → 1.2E-4(400f) → 1.8E-4(600f)
- **omega_f FULLY PLATEAUED at 8**: 12(100f) → 9(200f) → 8(400f) → 8(600f). omega_f=7 degrades.
- **Steps/frame efficiency trend SLOWS**: 600/f is insufficient at 600f (unlike 800/f at 400f). Need ≥700/f.
- F scales to 600f with NO diminishing returns (R²=0.99997 matches 400f ceiling)

## Batch 2 Configs (Iterations 37-40):

| Slot | omega_f | lr | Steps | Role |
|------|---------|-----|-------|------|
| 00 | 8.0 | **2.2E-4** | 420k | exploit — probe lr ceiling |
| 01 | **10.0** | **1.8E-4** | 420k | exploit — test omega_f upper at optimal lr |
| 02 | 8.0 | 1.8E-4 | **500k** | explore — more training (loss declining) |
| 03 | 8.0 | **1.8E-4** | **360k** | principle-test — high lr compensates fewer steps? |


============================================================
=== Batch 37-40 ===
============================================================
All tasks complete. Here's the summary:

## Block 5 Results (F@600f) — Iterations 37-40

| Iter | Mutation | R² | MSE | kino_SSIM | Time |
|------|----------|-----|-----|-----------|------|
| 37 | lr=2.2E-4 (exploit) | 0.999975 | 1.20E-5 | 0.9980 | 24.2min |
| **38** | **omega_f=10@lr=1.8E-4** | **0.999983** | **8.05E-6** | **0.9987** | **24.1min** |
| 39 | 500k steps (explore) | 0.999973 | 1.29E-5 | 0.9978 | 28.5min |
| 40 | lr=1.8E-4@360k (principle-test) | 0.999965 | 1.69E-5 | 0.9973 | 20.6min |

**Block 5 best**: Iter 38 — 256x4@omega_f=10@lr=1.8E-4@420k, R²=0.999983, 24.1min

### Key Block 5 Discoveries
1. **omega_f-lr INTERACTION**: omega_f=10 beats 8 at high lr (1.8E-4), but 8=10 at moderate lr. Higher lr unlocks higher frequency capacity.
2. **F POSITIVE data scaling to 600f**: R²=0.999983 EXCEEDS 400f best (0.99995).
3. **lr ceiling NOT found at 2.2E-4**: Wide tolerance [1.2-2.2]E-4.
4. **Overtraining confirmed**: 500k steps (833/f) no better than 420k (700/f).

### Block 6 Setup: Jp@600f siren_txy

| Slot | omega_f | lr | hidden_dim | total_steps | Dimension |
|------|---------|-----|------------|-------------|-----------|
| 00 | 5.0 | 2.5E-4 | 512 | 720k | Baseline |
| 01 | 4.0 | 2.5E-4 | 512 | 720k | omega_f (test downward shift) |
| 02 | 5.0 | 3E-4 | 512 | 720k | lr (probe ceiling) |
| 03 | 5.0 | 2.5E-4 | 384 | 720k | capacity (principle-test: 384 Pareto) |


============================================================
=== Batch 41-44 ===
============================================================
All tasks complete. Here's the summary:

## Batch 41-44 Results (Jp@600f, Block 6, Batch 1)

All 4 slots achieved **excellent** results (R² > 0.9998):

| Iter | Config | R² | MSE | slope | Time |
|------|--------|------|-----|-------|------|
| 41 | 512×3, omega=5, lr=2.5E-4 | 0.999833 | 6.01E-2 | 0.996 | 46.4min |
| 42 | 512×3, omega=**4**, lr=2.5E-4 | 0.999906 | 3.25E-2 | 0.998 | 46.6min |
| 43 | 512×3, omega=5, lr=**3E-4** | 0.999903 | 3.28E-2 | 0.998 | 46.4min |
| 44 | **384**×3, omega=5, lr=2.5E-4 | **0.999910** | **3.13E-2** | 0.998 | **31.6min** |

**Key findings:**
- **384 speed Pareto CONFIRMED at 600f** — iter 44 achieves best R², lowest MSE, and 32% faster training
- **omega_f=4** slightly improves over omega_f=5 at 600f — continuing the downward trend (5@400f → 4@600f)
- **lr=3E-4** is viable at 600f — lr ceiling rises: 2E-4(400f) → ≥3E-4(600f), matching ~1.5× per 1.5× frames
- Baseline (iter 41) was weakest — all mutations improved over it

## Next Batch (45-48) Configs Set:

| Slot | Strategy | Key change |
|------|----------|------------|
| 00 | Recombine | 384 + omega=4 (best of both mutations) |
| 01 | Recombine | 512 + omega=4 + lr=3E-4 |
| 02 | Speed probe | 384, steps 720k→540k (900/f) |
| 03 | Boundary test | 384, omega=3 (lower boundary) |


============================================================
=== Batch 45-48 ===
============================================================
All tasks complete. Here's the summary:

## Batch 2 Results (Iterations 45-48) — Block 6 END

| Slot | Iter | Config | R² | MSE | slope | Time | Rating |
|------|------|--------|-----|-----|-------|------|--------|
| 00 | 45 | 384×3@omega=4@lr=2.5E-4@720k | **0.999955** | **1.55E-2** | 0.999 | 31.7min | **BEST** |
| 01 | 46 | 512×3@omega=4@lr=3E-4@720k | 0.999913 | 3.08E-2 | 0.998 | 46.6min | Excellent |
| 02 | 47 | 384×3@omega=5@lr=2.5E-4@540k | 0.999912 | 3.05E-2 | 0.998 | **23.9min** | **Speed Pareto** |
| 03 | 48 | 384×3@omega=3@lr=2.5E-4@720k | 0.999954 | 1.55E-2 | 0.999 | 31.7min | Excellent |

**Key Block 6 findings:**
- **Jp@600f R²=0.999955** — excellent data scaling (near 400f quality with 50% more data)
- **omega_f peak BROADENS with data**: 400f narrow peak at 5 → 600f FLAT [3-4]. omega_f=3 matches omega_f=4 (0.99995 ≈ 0.99996), whereas at 400f omega_f=3 was 5× worse
- **384 unconditionally beats 512** at every tested config — speed Pareto is absolute
- **Recombination**: omega_f + capacity is additive; omega_f + lr is NOT additive

**Next block (Block 7):** C@600f siren_txy. All 4 configs set:
- Slot 00: omega=12, lr=5E-5, 896×3, 1.5M steps (baseline extrapolation)
- Slot 01: omega=15, lr=5E-5, 896×3, 1.5M steps (keep 400f omega)
- Slot 02: omega=12, lr=4E-5, 896×3, 1.5M steps (keep 400f lr)
- Slot 03: omega=12, lr=5E-5, 768×3, 1.5M steps (capacity principle-test)


============================================================
=== Batch 49-52 ===
============================================================
All tasks complete. Here's the summary:

## Batch 1 Results (Iterations 49-52): C@600f siren_txy

All 4 slots achieved **excellent** results (R² > 0.9998). C continues its upward data scaling trend.

| Iter | Config mutation | R² | MSE | Time |
|------|----------------|-----|-----|------|
| **49** | **Baseline** (omega=12, lr=5E-5, 896) | **0.999866** | 2.73E-2 | 240min |
| 50 | omega_f: 12→15 | 0.999849 | 3.08E-2 | 234min |
| 51 | lr: 5E-5→4E-5 | 0.999854 | 2.98E-2 | 234min |
| 52 | hidden_dim: 896→768 | 0.999818 | 3.72E-2 | 182min |

**Key findings:**
- **omega_f=12 CONFIRMED** over 15 — C follows downward trend: 25(100f)→20(200f)→15(400f)→12(600f)
- **lr=5E-5 marginally better** than 4E-5 — C lr-data scaling is weak (1.25× from 400→600f)
- **Capacity monotonicity CONFIRMED** at 600f — 896 > 768 (0.005% gap, 24% time savings for 768)
- **Loss still declining** at 1.5M steps — C does not overtrain, more steps may help
- C data scaling continues upward: 0.994(100f)→0.991(200f)→0.9998(400f)→**0.9999(600f)**

## Next Batch (Iterations 53-56) configured:
- **Slot 00**: omega_f=10 (continue downward probe)
- **Slot 01**: lr=6E-5 (lr ceiling probe)
- **Slot 02**: 2M steps (test if more training helps)
- **Slot 03**: hidden_dim=1024 (capacity scaling probe)

